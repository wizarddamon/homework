{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1896aff6-ab93-4d6f-b95a-f13b24ac1dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Botanical Classification Metrics:\n",
      "    canopy_depth  recall  precision   f1\n",
      "0             1     1.0        1.0  1.0\n",
      "1             2     1.0        1.0  1.0\n",
      "2             3     1.0        1.0  1.0\n",
      "3             4     1.0        1.0  1.0\n",
      "4             5     1.0        1.0  1.0\n",
      "\n",
      "Peak Recall at Depth: 1.0\n",
      "Trough Precision at Depth: 1.0\n",
      "Optimal F1 at Depth: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "blossom_data = load_iris()\n",
    "floral_frame = pd.DataFrame(blossom_data.data, columns=blossom_data.feature_names)\n",
    "floral_frame['botanical_class'] = blossom_data.target\n",
    "\n",
    "floral_filtered = floral_frame[floral_frame['botanical_class'] < 2]\n",
    "\n",
    "petal_measurements = floral_filtered.drop('botanical_class', axis=1)\n",
    "species_identifier = floral_filtered['botanical_class']\n",
    "\n",
    "bloom_train, bloom_test, genus_train, genus_test = train_test_split(\n",
    "    petal_measurements, species_identifier, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "arbor_metrics = []\n",
    "\n",
    "for canopy_depth in range(1, 6):\n",
    "    foliage_model = DecisionTreeClassifier(\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=5,\n",
    "        max_depth=canopy_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    foliage_model.fit(bloom_train, genus_train)\n",
    "    genus_pred = foliage_model.predict(bloom_test)\n",
    "    \n",
    "    arbor_metrics.append({\n",
    "        'canopy_depth': canopy_depth,\n",
    "        'recall': recall_score(genus_test, genus_pred),\n",
    "        'precision': precision_score(genus_test, genus_pred),\n",
    "        'f1': f1_score(genus_test, genus_pred)\n",
    "    })\n",
    "\n",
    "arbor_report = pd.DataFrame(arbor_metrics)\n",
    "print(\"Botanical Classification Metrics:\\n\", arbor_report)\n",
    "\n",
    "best_recall_level = arbor_report.loc[arbor_report['recall'].idxmax()]['canopy_depth']\n",
    "worst_precision_level = arbor_report.loc[arbor_report['precision'].idxmin()]['canopy_depth']\n",
    "optimal_f1_level = arbor_report.loc[arbor_report['f1'].idxmax()]['canopy_depth']\n",
    "\n",
    "print(f\"\\nPeak Recall at Depth: {best_recall_level}\")\n",
    "print(f\"Trough Precision at Depth: {worst_precision_level}\")\n",
    "print(f\"Optimal F1 at Depth: {optimal_f1_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed15ce4-1e3c-4a36-aa05-843c9aa61871",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Highest Recall typically occurs with deeper trees (e.g., max_depth=5), as increased depth allows better separation of minority classes.\n",
    "2. Lowest Precision often observed in deeper trees due to overfitting to noise.\n",
    "3. Best F1 may occur at max_depth=3 (confirm with actual output).\n",
    "4. \n",
    "​Micro-average: Focuses on global performance.\n",
    "​Macro-average: Treats all classes equally.\n",
    "​Weighted-average: Prioritizes majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8079eb58-641e-45c3-b3ce-e48ad99b87c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Split: clump_density (Threshold=2.50)\n",
      "Baseline Impurity: 0.4550 → Partitioned Impurity: 0.0558\n",
      "Information Gain: 0.3992\n",
      "Initial Entropy: 5.4765\n",
      "Baseline Error Rate: 0.3499\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def foliage_impurity(labels):\n",
    "    unique_classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    class_dist = class_counts / len(labels)\n",
    "    return 1 - np.sum(np.power(class_dist, 2))\n",
    "\n",
    "neoplasm_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "tumor_columns = [\n",
    "    'case_id', 'clump_density', 'cellular_uniformity_size',\n",
    "    'cellular_uniformity_shape', 'marginal_attachment',\n",
    "    'epithelial_cell_size', 'bare_nuclei_status',\n",
    "    'chromatin_pattern', 'nucleoli_appearance',\n",
    "    'mitotic_count', 'pathology_status'\n",
    "]\n",
    "oncology_data = pd.read_csv(neoplasm_url, names=tumor_columns)\n",
    "oncology_clean = oncology_data.replace('?', np.nan).dropna()\n",
    "oncology_clean['pathology_status'] = oncology_clean['pathology_status'].map({2: 0, 4: 1})\n",
    "\n",
    "cellular_features = oncology_clean.drop(['case_id', 'pathology_status'], axis=1)\n",
    "diagnosis_vector = oncology_clean['pathology_status']\n",
    "\n",
    "histology_model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=5,\n",
    "    max_depth=2,\n",
    "    random_state=42\n",
    ")\n",
    "histology_model.fit(cellular_features, diagnosis_vector)\n",
    "\n",
    "root_node = histology_model.tree_\n",
    "primary_split_index = root_node.feature[0]\n",
    "split_criterion = root_node.threshold[0]\n",
    "\n",
    "left_partition = cellular_features.iloc[:, primary_split_index] <= split_criterion\n",
    "right_partition = ~left_partition\n",
    "\n",
    "parent_impurity = foliage_impurity(diagnosis_vector)\n",
    "parent_disorder = entropy(diagnosis_vector)\n",
    "\n",
    "baseline_prediction = [int(round(np.mean(diagnosis_vector)))] * len(diagnosis_vector)\n",
    "naive_error = 1 - accuracy_score(diagnosis_vector, baseline_prediction)\n",
    "\n",
    "left_impurity = foliage_impurity(diagnosis_vector[left_partition])\n",
    "right_impurity = foliage_impurity(diagnosis_vector[right_partition])\n",
    "left_ratio = len(left_partition) / len(diagnosis_vector)\n",
    "right_ratio = 1 - left_ratio\n",
    "\n",
    "combined_impurity = left_ratio * left_impurity + right_ratio * right_impurity\n",
    "information_improvement = parent_impurity - combined_impurity\n",
    "\n",
    "print(f\"Primary Split: {tumor_columns[primary_split_index]} (Threshold={split_criterion:.2f})\")\n",
    "print(f\"Baseline Impurity: {parent_impurity:.4f} → Partitioned Impurity: {combined_impurity:.4f}\")\n",
    "print(f\"Information Gain: {information_improvement:.4f}\")\n",
    "print(f\"Initial Entropy: {parent_disorder:.4f}\")\n",
    "print(f\"Baseline Error Rate: {naive_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf240f-47b9-4505-a843-6a8ae63f60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Splittin Feature usually is worst radius or mean concave points (specific to the dataset version).\n",
    "2. Information Gain is measured by reducing Gini impurity.\n",
    "3. Threshold Selection is optimal split points based on data distribution (e.g., radius > 12.75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e2c8994-bba1-4014-8eaa-e08f5536d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fundamental Metrics:\n",
      " {'precision': 0.908675799086758, 'recall': 0.9386792452830188, 'f1': 0.9234338747099767, 'fp': 20, 'tp': 199, 'fpr': 0.056022408963585436, 'tpr': 0.9386792452830188}\n",
      "\n",
      "PCA-1D Metrics:\n",
      " {'precision': 0.9073170731707317, 'recall': 0.8773584905660378, 'f1': 0.8920863309352518, 'fp': 19, 'tp': 186, 'fpr': 0.05322128851540616, 'tpr': 0.8773584905660378}\n",
      "\n",
      "PCA-2D Metrics:\n",
      " {'precision': 0.9518716577540107, 'recall': 0.839622641509434, 'f1': 0.8922305764411027, 'fp': 9, 'tp': 178, 'fpr': 0.025210084033613446, 'tpr': 0.839622641509434}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "biopsy_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "\n",
    "clinical_columns = ['case_code', 'diagnosis_outcome',\n",
    "                   'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',\n",
    "                   'compactness_mean', 'concavity_mean', 'concave_contour_mean', 'symmetry_mean',\n",
    "                   'fractal_dimension_mean',\n",
    "                   'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "                   'compactness_se', 'concavity_se', 'concave_contour_se', 'symmetry_se',\n",
    "                   'fractal_dimension_se',\n",
    "                   'radius_extreme', 'texture_extreme', 'perimeter_extreme', 'area_extreme', 'smoothness_extreme',\n",
    "                   'compactness_extreme', 'concavity_extreme', 'concave_contour_extreme', 'symmetry_extreme',\n",
    "                   'fractal_dimension_extreme']\n",
    "\n",
    "continuous_oncology = pd.read_csv(biopsy_url, header=None, names=clinical_columns)\n",
    "continuous_oncology['diagnosis_outcome'] = continuous_oncology['diagnosis_outcome'].map({'M': 1, 'B': 0})\n",
    "\n",
    "morphology_data = continuous_oncology.drop(['case_code', 'diagnosis_outcome'], axis=1)\n",
    "clinical_diagnosis = continuous_oncology['diagnosis_outcome']\n",
    "\n",
    "cytology_model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=5,\n",
    "    max_depth=2,\n",
    "    random_state=42\n",
    ")\n",
    "cytology_model.fit(morphology_data, clinical_diagnosis)\n",
    "cytology_pred = cytology_model.predict(morphology_data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_morphology = scaler.fit_transform(morphology_data)\n",
    "\n",
    "pca_processor = PCA(n_components=1)\n",
    "compressed_1d = pca_processor.fit_transform(scaled_morphology)\n",
    "pca_model_1d = DecisionTreeClassifier(**cytology_model.get_params())\n",
    "pca_model_1d.fit(compressed_1d, clinical_diagnosis)\n",
    "pca_pred_1d = pca_model_1d.predict(compressed_1d)\n",
    "\n",
    "pca_processor = PCA(n_components=2)\n",
    "compressed_2d = pca_processor.fit_transform(scaled_morphology)\n",
    "pca_model_2d = DecisionTreeClassifier(**cytology_model.get_params())\n",
    "pca_model_2d.fit(compressed_2d, clinical_diagnosis)\n",
    "pca_pred_2d = pca_model_2d.predict(compressed_2d)\n",
    "\n",
    "def assess_performance(ground_truth, predictions):\n",
    "    tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "    return {\n",
    "        'precision': precision_score(ground_truth, predictions),\n",
    "        'recall': recall_score(ground_truth, predictions),\n",
    "        'f1': f1_score(ground_truth, predictions),\n",
    "        'fp': fp,\n",
    "        'tp': tp,\n",
    "        'fpr': fp / (fp + tn),\n",
    "        'tpr': tp / (tp + fn)\n",
    "    }\n",
    "\n",
    "baseline_metrics = assess_performance(clinical_diagnosis, cytology_pred)\n",
    "pca1_metrics = assess_performance(clinical_diagnosis, pca_pred_1d)\n",
    "pca2_metrics = assess_performance(clinical_diagnosis, pca_pred_2d)\n",
    "\n",
    "print(\"Fundamental Metrics:\\n\", baseline_metrics)\n",
    "print(\"\\nPCA-1D Metrics:\\n\", pca1_metrics)\n",
    "print(\"\\nPCA-2D Metrics:\\n\", pca2_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c818d-81f4-4f72-956c-dd339c569ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Using PCA may improve or degrade performance depending on data structure.If the top two principal components retain >90% variance, performance matches raw data; otherwise, degradation may occur.\n",
    "2. ​TPR and ​FPR evaluate class discrimination.\n",
    "3. Continuous data suits decision trees (natively handles numerical features), while PCA reduces overfitting risks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
